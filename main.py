import feedparser
from jinja2 import Environment, FileSystemLoader
from datetime import datetime
import pytz
import os
import time
import urllib.request
from urllib.parse import urlparse
from itertools import groupby
from operator import itemgetter
import re

# --- é…ç½®åŒº (é‡‡çº³æ‚¨çš„å»ºè®®ï¼Œä½¿ç”¨ç¨³å®šæœåŠ¡å™¨åˆ—è¡¨) ---
STABLE_RSSHUB_SERVERS = [
    'https://rsshub.rss.tips',
    'https://rsshub.pseudoyu.com',
    'https://rsshub.ktachibana.party',
    'https://rsshub.asailor.org',
    'https://rsshub.email-once.com',
]

# æˆ‘ä»¬å°†åœ¨è¿™é‡Œå®šä¹‰RSSæºçš„â€œè·¯å¾„â€ï¼Œè€Œä¸æ˜¯å†™æ­»çš„å®Œæ•´URL
RSS_PATHS = {
    'AO3 | ä½é¸£ä½': 'https://archiveofourown.org/tags/14303/feed.atom', # è¿™æ˜¯å®Œæ•´URLï¼Œç‰¹æ®Šå¤„ç†
    'Bç«™ | ä½åŠ©': '/bilibili/vsearch/ä½åŠ©/pubdate/0/1',
    'Bç«™ | é¸£ä½': '/bilibili/vsearch/é¸£ä½/pubdate/0/1',
    'Bç«™ | ä½é¸£': '/bilibili/vsearch/ä½é¸£/pubdate/0/1',
    'å¾®åšè¶…è¯ | ä½é¸£': '/weibo/super_index/10080800f63e66b38b96a8ca5ecb2e0b3cfae4/sort_time',
    'å¾®åšè¶…è¯ | é¸£ä½': '/weibo/super_index/100808799b9b6da0d5d6d2f398c771b28b4039/sort_time',
}
MAX_ENTRIES_PER_SOURCE = 20

# --- AO3 ç¿»è¯‘å­—å…¸ (ä¿æŒä¸å˜) ---
AO3_RATING_TRANSLATIONS = {
    "Not Rated": "æœªåˆ†çº§", "General Audiences": "å…¨å¹´é¾„", "Teen And Up Audiences": "é’å°‘å¹´åŠä»¥ä¸Š",
    "Mature": "æˆäººçº§", "Explicit": "é™åˆ¶çº§"
}
AO3_WARNING_TRANSLATIONS = {
    "Creator Chose Not To Use Archive Warnings": "ä½œè€…é€‰æ‹©ä¸ä½¿ç”¨ä½œå“å­˜æ¡£è­¦å‘Š", "No Archive Warnings Apply": "æ— å­˜æ¡£è­¦å‘Š",
    "Graphic Depictions Of Violence": "æ¶‰åŠæš´åŠ›å†…å®¹", "Major Character Death": "ä¸»è¦è§’è‰²æ­»äº¡",
    "Rape/Non-Con": "å¼ºæš´/éè‡ªæ„¿æ€§è¡Œä¸º", "Underage": "æœªæˆå¹´", "Underage Sex": "æœªæˆå¹´æ€§è¡Œä¸º"
}


def try_fetch_with_failover(path):
    """
    ä¸ºRSSHubæºå®ç°è‡ªåŠ¨æ•…éšœåˆ‡æ¢åŠŸèƒ½ã€‚
    å®ƒä¼šéå†STABLE_RSSHUB_SERVERSåˆ—è¡¨ï¼Œç›´åˆ°æˆåŠŸæˆ–å…¨éƒ¨å¤±è´¥ã€‚
    """
    for server in STABLE_RSSHUB_SERVERS:
        url_to_try = f"{server}{path}"
        try:
            print(f"  â†ªï¸ æ­£åœ¨å°è¯•æœåŠ¡å™¨: {server} ...")
            feed = feedparser.parse(url_to_try)
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸï¼šæœ‰å†…å®¹ã€ä¸æ˜¯bozoï¼ˆæ ¼å¼é”™è¯¯ï¼‰ã€æœ‰æ¡ç›®
            if feed and not feed.bozo and feed.entries:
                print(f"  âœ… æˆåŠŸä» {server} è·å–æ•°æ®ã€‚")
                return feed # æˆåŠŸè·å–ï¼Œç«‹å³è¿”å›ç»“æœ
        except Exception as e:
            print(f"  âŒ è¿æ¥åˆ° {server} å¤±è´¥: {e}")
            continue # å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
    
    print(f"  ğŸ›‘ è­¦å‘Š: æ‰€æœ‰å¤‡ç”¨æœåŠ¡å™¨å‡æ— æ³•æˆåŠŸæŠ“å–è·¯å¾„ {path}")
    return None # æ‰€æœ‰æœåŠ¡å™¨éƒ½å¤±è´¥äº†ï¼Œè¿”å›None


def fetch_all_feeds():
    all_entries = []
    print("ğŸ›°ï¸ å¼€å§‹è¿½è¸ªæœˆäº®è½¨è¿¹...")
    
    for name, path_or_url in RSS_PATHS.items():
        print(f"\n----- æ­£åœ¨æ£€æŸ¥æº: {name} -----")
        
        feed = None
        # åˆ¤æ–­æ˜¯éœ€è¦æ•…éšœåˆ‡æ¢çš„RSSHubè·¯å¾„ï¼Œè¿˜æ˜¯ç›´æ¥çš„URL
        if path_or_url.startswith('/'): # æˆ‘ä»¬çš„RSSHubè·¯å¾„éƒ½ä»¥'/'å¼€å¤´
            feed = try_fetch_with_failover(path_or_url)
        else: # è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„URLï¼Œæ¯”å¦‚AO3
            try:
                print(f"  â†ªï¸ ç›´æ¥æŠ“å–æº: {path_or_url}")
                feed = feedparser.parse(path_or_url)
            except Exception as e:
                 print(f"ğŸ›‘ ç›´æ¥æŠ“å–æº '{name}' å¤±è´¥: {e}")

        if not feed or (feed.bozo and not feed.entries): # å¦‚æœæŠ“å–å¤±è´¥æˆ–å†…å®¹ä¸ºç©º
            if feed and feed.bozo:
                print(f"âš ï¸ è­¦å‘Š: '{name}' çš„Feedæ ¼å¼ä¸è§„èŒƒæˆ–æŠ“å–å¤±è´¥ã€‚è¯¦æƒ…: {feed.bozo_exception}")
            continue # è·³è¿‡æ­¤æº
            
        print(f"æŠ“å–åˆ° {len(feed.entries)} æ¡åŸå§‹å†…å®¹ã€‚")
        
        filtered_count = 0
        for entry in feed.entries:
            if 'AO3' in name and 'Language: ä¸­æ–‡' not in entry.summary:
                continue

            bilibili_meta = ao3_meta = None
            thumbnail, clean_summary = '', entry.summary
            author = entry.get('author')
            categories = [tag.term for tag in entry.get('tags', [])]

            if 'Bç«™' in name:
                bilibili_meta = {
                    'length': (re.search(r'Length:\s*([\d:]+)', entry.summary) or ['-', '-'])[1],
                    'play': (re.search(r'Play:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                    'favorite': (re.search(r'Favorite:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                    'danmaku': (re.search(r'Danmaku:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                    'comment': (re.search(r'Comment:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                }
                meta_keywords = ['Length:', 'AuthorID:', 'Play:', 'Favorite:', 'Danmaku:', 'Comment:', 'Match By:']
                meta_start_index = -1
                for keyword in meta_keywords:
                    found_index = entry.summary.find(keyword)
                    if found_index != -1 and (meta_start_index == -1 or found_index < meta_start_index):
                        meta_start_index = found_index
                clean_summary = entry.summary[:meta_start_index].strip() if meta_start_index != -1 else entry.summary
                clean_summary = clean_summary.replace(entry.title, '').strip()
                if entry.description and '<img src="' in entry.description:
                    start = entry.description.find('<img src="') + 10
                    end = entry.description.find('"', start)
                    thumbnail = 'https:' + entry.description[start:end] if entry.description[start:end].startswith('//') else entry.description[start:end]
            
            elif 'AO3' in name:
                summary_html = entry.summary
                original_rating = (re.search(r'Rating:.*?>(.*?)<\/a>', summary_html) or ['-','-'])[1]
                original_warning = (re.search(r'Warnings:.*?>(.*?)<\/a>', summary_html) or ['-','-'])[1]
                translated_rating = AO3_RATING_TRANSLATIONS.get(original_rating, original_rating)
                translated_warning = AO3_WARNING_TRANSLATIONS.get(original_warning, original_warning)
                ao3_meta = {
                    'words': (re.search(r'Words:\s*([\d,]+)', summary_html) or ['-','-'])[1],
                    'chapters': (re.search(r'Chapters:\s*([\d/]+)', summary_html) or ['-','-'])[1],
                    'rating': translated_rating, 'warnings': translated_warning,
                    'relationships': [rel.replace('*s*', '/') for rel in re.findall(r'relationships/.*?">(.*?)<\/a>', summary_html)],
                }
                clean_summary_match = re.search(r'<p>(?!by)(.*?)<\/p>', summary_html, re.DOTALL)
                clean_summary = clean_summary_match.group(1).strip() if clean_summary_match else ''

            parsed_time = None
            time_struct = entry.get('published_parsed') or entry.get('updated_parsed')
            if time_struct:
                dt_naive = datetime.fromtimestamp(time.mktime(time_struct))
                parsed_time = pytz.utc.localize(dt_naive).astimezone(pytz.timezone('Asia/Shanghai'))

            all_entries.append({
                'source': name, 'title': entry.title, 'link': entry.link, 'author': author,
                'published': parsed_time, 'summary': clean_summary, 'thumbnail': thumbnail,
                'categories': categories, 'bilibili_meta': bilibili_meta, 'ao3_meta': ao3_meta,
            })
            filtered_count += 1
            if filtered_count >= MAX_ENTRIES_PER_SOURCE: break
        print(f"ç»è¿‡æ»¤å’Œå¤„ç†åï¼Œä¿ç•™ {filtered_count} æ¡æœ‰æ•ˆå†…å®¹ã€‚")

    all_entries.sort(key=lambda x: x['published'] or datetime.min.replace(tzinfo=pytz.utc), reverse=True)
    print(f"\nâœ… è¿½è¸ªå®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_entries)} æ¡æœ‰æ•ˆè®°å½•ã€‚")
    return all_entries

def generate_html(entries):
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆè§‚æµ‹æŠ¥å‘Š (HTML)...")
    grouped_entries = {}
    entries.sort(key=itemgetter('source'))
    for source_name, group in groupby(entries, key=itemgetter('source')):
        grouped_entries[source_name] = sorted(list(group), key=lambda x: x['published'] or datetime.min.replace(tzinfo=pytz.utc), reverse=True)
    
    env = Environment(loader=FileSystemLoader('.'))
    template = env.get_template('template.html')
    update_time = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
    
    html_content = template.render(grouped_entries=grouped_entries, update_time=update_time, site_title="Lunar Orbit Observatory", site_slogan="Tracing the Path of the Moon.")
    
    return html_content

if __name__ == "__main__":
    entries = fetch_all_feeds()
    output_content = generate_html(entries)
    
    output_file_path = "index.html"
    with open(output_file_path, "w", encoding="utf-8") as f:
        f.write(output_content)
    
    print(f"âœ… è§‚æµ‹æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼Œå·²ä¿å­˜ä¸º {output_file_path}ã€‚")
