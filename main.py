import feedparser
from jinja2 import Environment, FileSystemLoader
from datetime import datetime
import pytz
import os
import time
import urllib.request
from itertools import groupby
from operator import itemgetter
import re

# --- é…ç½®åŒº ---
RSSHUB_SERVER = 'https://rsshub.rss.tips'
RSS_FEEDS = {
    'AO3 | ä½é¸£ä½': 'https://archiveofourown.org/tags/14303/feed.atom',
    'Bç«™ | ä½åŠ©': f'{RSSHUB_SERVER}/bilibili/vsearch/ä½åŠ©/pubdate/0/1',
    'Bç«™ | é¸£ä½': f'{RSSHUB_SERVER}/bilibili/vsearch/é¸£ä½/pubdate/0/1',
    'Bç«™ | ä½é¸£': f'{RSSHUB_SERVER}/bilibili/vsearch/ä½é¸£/pubdate/0/1',
    'å¾®åšè¶…è¯ | ä½é¸£': 'https://rsshub.rss.tips/weibo/super_index/10080800f63e66b38b96a8ca5ecb2e0b3cfae4/sort_time',
    'å¾®åšè¶…è¯ | é¸£ä½': 'https://rsshub.rss.tips/weibo/super_index/100808799b9b6da0d5d6d2f398c771b28b4039/sort_time',
    #'LOFTER | ä½åŠ©': f'{RSSHUB_SERVER}/lofter/tag/ä½åŠ©',
    #'LOFTER | é¸£ä½': f'{RSSHUB_SERVER}/lofter/tag/é¸£ä½',
    #'LOFTER | ä½é¸£': f'{RSSHUB_SERVER}/lofter/tag/ä½é¸£',
}
MAX_ENTRIES_PER_SOURCE = 20

# --- AO3 ç¿»è¯‘å­—å…¸ ---
AO3_RATING_TRANSLATIONS = {
    "Not Rated": "æœªåˆ†çº§",
    "General Audiences": "å…¨å¹´é¾„",
    "Teen And Up Audiences": "é’å°‘å¹´åŠä»¥ä¸Š",
    "Mature": "æˆäººçº§",
    "Explicit": "é™åˆ¶çº§"
}
AO3_WARNING_TRANSLATIONS = {
    "Creator Chose Not To Use Archive Warnings": "ä½œè€…é€‰æ‹©ä¸ä½¿ç”¨ä½œå“å­˜æ¡£è­¦å‘Š",
    "No Archive Warnings Apply": "æ— å­˜æ¡£è­¦å‘Š",
    "Graphic Depictions Of Violence": "æ¶‰åŠæš´åŠ›å†…å®¹",
    "Major Character Death": "ä¸»è¦è§’è‰²æ­»äº¡",
    "Rape/Non-Con": "å¼ºæš´/éè‡ªæ„¿æ€§è¡Œä¸º",
    "Underage": "æœªæˆå¹´",
    "Underage Sex": "æœªæˆå¹´æ€§è¡Œä¸º"
}

# ... setup_proxy() å‡½æ•°ä¿æŒä¸å˜ ...
def setup_proxy():
    proxy_address = 'http://127.0.0.1:7890'
    if not proxy_address: return
    proxy_handler = urllib.request.ProxyHandler({'http': proxy_address, 'https': proxy_address})
    opener = urllib.request.build_opener(proxy_handler)
    urllib.request.install_opener(opener)

def fetch_all_feeds():
    all_entries = []
    print("ğŸ›°ï¸ å¼€å§‹è¿½è¸ªæœˆäº®è½¨è¿¹...")
    setup_proxy()
    for name, url in RSS_FEEDS.items():
        print(f"\n----- æ­£åœ¨æ£€æŸ¥æº: {name} -----")
        try:
            feed = feedparser.parse(url)
            if feed.bozo:
                print(f"âš ï¸ è­¦å‘Š: '{name}' çš„Feedæ ¼å¼ä¸è§„èŒƒæˆ–æŠ“å–å¤±è´¥ã€‚è¯¦æƒ…: {feed.bozo_exception}")
                continue
            print(f"æŠ“å–åˆ° {len(feed.entries)} æ¡åŸå§‹å†…å®¹ã€‚")
            
            filtered_count = 0
            for entry in feed.entries:
                if 'AO3' in name and 'Language: ä¸­æ–‡' not in entry.summary:
                    continue

                bilibili_meta = ao3_meta = None
                thumbnail, clean_summary = '', entry.summary
                author = entry.get('author')
                categories = [tag.term for tag in entry.get('tags', [])]

                if 'Bç«™' in name:
                    bilibili_meta = {
                        'length': (re.search(r'Length:\s*([\d:]+)', entry.summary) or ['-', '-'])[1],
                        'play': (re.search(r'Play:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                        'favorite': (re.search(r'Favorite:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                        'danmaku': (re.search(r'Danmaku:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                        'comment': (re.search(r'Comment:\s*([\d,]+)', entry.summary) or ['-', '-'])[1],
                    }
                    meta_keywords = ['Length:', 'AuthorID:', 'Play:', 'Favorite:', 'Danmaku:', 'Comment:', 'Match By:']
                    meta_start_index = -1
                    for keyword in meta_keywords:
                        found_index = entry.summary.find(keyword)
                        if found_index != -1 and (meta_start_index == -1 or found_index < meta_start_index):
                            meta_start_index = found_index
                    clean_summary = entry.summary[:meta_start_index].strip() if meta_start_index != -1 else entry.summary
                    clean_summary = clean_summary.replace(entry.title, '').strip()
                    if entry.description and '<img src="' in entry.description:
                        start = entry.description.find('<img src="') + 10
                        end = entry.description.find('"', start)
                        thumbnail = 'https:' + entry.description[start:end] if entry.description[start:end].startswith('//') else entry.description[start:end]
                
                elif 'AO3' in name:
                    summary_html = entry.summary
                    
                    original_rating = (re.search(r'Rating:.*?>(.*?)<\/a>', summary_html) or ['-','-'])[1]
                    original_warning = (re.search(r'Warnings:.*?>(.*?)<\/a>', summary_html) or ['-','-'])[1]

                    translated_rating = AO3_RATING_TRANSLATIONS.get(original_rating, original_rating)
                    translated_warning = AO3_WARNING_TRANSLATIONS.get(original_warning, original_warning)

                    ao3_meta = {
                        # --- ä¿®å¤ç‚¹ï¼šåœ¨è¿™é‡Œè¡¥ä¸Šäº†ç¼ºå¤±çš„ç»“å°¾å•å¼•å· ---
                        'words': (re.search(r'Words:\s*([\d,]+)', summary_html) or ['-','-'])[1],
                        'chapters': (re.search(r'Chapters:\s*([\d/]+)', summary_html) or ['-','-'])[1],
                        'rating': translated_rating,
                        'warnings': translated_warning,
                        'relationships': [rel.replace('*s*', '/') for rel in re.findall(r'relationships/.*?">(.*?)<\/a>', summary_html)],
                    }
                    clean_summary_match = re.search(r'<p>(?!by)(.*?)<\/p>', summary_html, re.DOTALL)
                    clean_summary = clean_summary_match.group(1).strip() if clean_summary_match else ''

                parsed_time = None
                time_struct = entry.get('published_parsed') or entry.get('updated_parsed')
                if time_struct:
                    dt_naive = datetime.fromtimestamp(time.mktime(time_struct))
                    parsed_time = pytz.utc.localize(dt_naive).astimezone(pytz.timezone('Asia/Shanghai'))

                all_entries.append({
                    'source': name, 'title': entry.title, 'link': entry.link, 'author': author,
                    'published': parsed_time, 'summary': clean_summary, 'thumbnail': thumbnail,
                    'categories': categories, 'bilibili_meta': bilibili_meta, 'ao3_meta': ao3_meta,
                })
                filtered_count += 1
                if filtered_count >= MAX_ENTRIES_PER_SOURCE: break
            print(f"ç»è¿‡æ»¤å’Œå¤„ç†åï¼Œä¿ç•™ {filtered_count} æ¡æœ‰æ•ˆå†…å®¹ã€‚")
        except Exception as e:
            print(f"ğŸ›‘ å‘ç”Ÿä¸¥é‡ç½‘ç»œé”™è¯¯ï¼Œæ— æ³•å¤„ç†æº '{name}': {e}")
    all_entries.sort(key=lambda x: x['published'] or datetime.min.replace(tzinfo=pytz.utc), reverse=True)
    print(f"\nâœ… è¿½è¸ªå®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_entries)} æ¡æœ‰æ•ˆè®°å½•ã€‚")
    return all_entries

# ... generate_html() å’Œ __main__ éƒ¨åˆ†å®Œå…¨ä¸å˜ ...
def generate_html(entries):
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆè§‚æµ‹æŠ¥å‘Š (HTML)...")
    grouped_entries = {}
    entries.sort(key=itemgetter('source'))
    for source_name, group in groupby(entries, key=itemgetter('source')):
        grouped_entries[source_name] = sorted(list(group), key=lambda x: x['published'] or datetime.min.replace(tzinfo=pytz.utc), reverse=True)
    env = Environment(loader=FileSystemLoader('.'))
    template = env.get_template('template.html')
    update_time = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
    html_content = template.render(grouped_entries=grouped_entries, update_time=update_time, site_title="Lunar Orbit Observatory", site_slogan="Tracing the Path of the Moon.")
    output_dir = "dist"
    if not os.path.exists(output_dir): os.makedirs(output_dir)
    with open(os.path.join(output_dir, 'index.html'), 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæ¯•: {os.path.join(output_dir, 'index.html')}")

if __name__ == "__main__":
    entries = fetch_all_feeds()
    generate_html(entries)

